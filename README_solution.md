**Deliverables**

$ Captured outputs in csv files
$ Committed code to repo
$ If there are over 1M attractions to capture:
    1. Use Multithreading, Multiprocessing
    2. Use Generators where necessary so as to not to store all the URLs/data in memory
    (Python generators are a simple way of creating iterators. All the overhead are automatically handled by generators in Python. Simply speaking, a generator is a function that returns an object (iterator) which we can iterate over (one value at a time))
    3. Use lxml(written in C) and beautifulsoup together for better performance
    4. Profile code to identify bottlenecks and optimize accordingly




